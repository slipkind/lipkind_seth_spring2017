{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  # Import to get api-key environment variable\n",
    "import requests # Import to request data from ny times\n",
    "import json\n",
    "from time import sleep\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import itertools\n",
    "import collections\n",
    "import csv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33bbafed080c4b6c9cf9272dceb30985\n"
     ]
    }
   ],
   "source": [
    "# print (nytimes) # shows the nytimes api key that I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nytimes = os.getenv('ny_times_api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#booksearch = \"https://api.nytimes.com/svc/books/v3/lists/\"\n",
    "#booksearch2 = \"https://api.nytimes.com/svc/books/v3/lists/overview\"\n",
    "articlesearch = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\" # article search api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllRequests(pageNumber): # article search\n",
    "    request_params = { # request parameters dict used for the get method from the requests module\n",
    "        \"api-key\"    : nytimes,\n",
    "        \"page\"       : pageNumber\n",
    "    }\n",
    "    response = requests.get(articlesearch, request_params)\n",
    "    sleep(10) # sleep so the nytimes api isn't pinged to fast\n",
    "    with open('ArticleSearchPage' + str(pageNumber) + '.json', 'w', errors='ignore') as f:\n",
    "        json.dump(response.json(), f, ensure_ascii=False)\n",
    "\n",
    "#         \"fq\"         : 'source.contains:(\"New York Times\")',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllArchives(year):\n",
    "    archivesearch = \"https://api.nytimes.com/svc/archive/v1/\" + str(year) + \"/1.json\" # archive search api call\n",
    "    request_params = { # request parameters dict used for the get method from the requests module\n",
    "        \"api-key\"    : nytimes\n",
    "    }\n",
    "    response = requests.get(archivesearch, request_params)\n",
    "    sleep(10) # sleep so the nytimes api isn't pinged to fast\n",
    "    with open('Archive' + str(year) + '.json', 'w', errors='ignore') as f:\n",
    "        json.dump(response.json(), f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i < 21:\n",
    "    getAllRequests(i) # get article searches page 1-20\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "i = 2010\n",
    "while i < 2018:\n",
    "    getAllArchives(i) # get archives search 2010-2017\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_articles = glob ('Article*') # get all article api json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_article_dict = dict() # dict for all json articles\n",
    "for article in all_articles:\n",
    "    file = open(article).read()\n",
    "    json_file = json.loads(file)\n",
    "    my_article_dict[file] = json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First analysis of Articles:\n",
    "- This analysis was to see which article had the most words on the first 20 pages of the articles api on 03/03/2017\n",
    "- Each page had responses which contained documents\n",
    "- Each document had a headline and a word count\n",
    "- Using the two keys above, I was able to create a dictionary that was later sorted based on word count\n",
    "- CONCLUSION:\n",
    "    - \"Inside Sara Berman's closet has the most words of any article written from the first 20 pages as of 03/03/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count_dict = dict()\n",
    "for my_key in my_article_dict:\n",
    "    for doc in my_article_dict[my_key][\"response\"][\"docs\"]:\n",
    "        word_count_dict[doc['headline']['main']] = doc['word_count']\n",
    "sorted_list = sorted(word_count_dict.items(), key=lambda pair: pair[1], reverse=True)\n",
    "#print (sorted_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ana1art = 'Ana1Articles'\n",
    "if not os.path.exists(ana1art):\n",
    "    os.mkdir(ana1art)\n",
    "with open(ana1art + '/ArticlesWordCount.csv', 'w', newline='', ) as csvfile: # open the csvfile as a writeable file\n",
    "    fieldname = ['word', 'frequency']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldname) # uses dictWriter to make columns in the csv file\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in sorted_list: # for to search through each dict pair in the list\n",
    "        writer.writerow({'word': i[0], 'frequency': i[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Analysis of Articles API:\n",
    "- There were 190 articles printed in the first 20 pages of the articles api on 03/03/2017\n",
    "- Of that list: 70 were from the 'Associated Press', 62 were from 'Reuters', 56 were from 'The New York Times', 1 was from 'CNBC', and 1 was from 'Internation New York Times'\n",
    "- This shows that the New York Times does not print as often as either the Associated Press (1st) or Reuters (2nd)\n",
    "- CONCLUSION:\n",
    "    - The New York Times writes only the third most articles, indicting either there is more staff at Associated Press and Reuters, or The New York Times does not bring in as much money as the first two elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_list = list()\n",
    "for my_key in my_article_dict:\n",
    "    for doc in my_article_dict[my_key][\"response\"][\"docs\"]:\n",
    "        source_list.append(doc['source'])\n",
    "source_set = set(source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_dict = dict()\n",
    "for source in source_set:\n",
    "    source_count = source_list.count(source)\n",
    "    count_dict[source] = source_count\n",
    "sorted_sources = sorted(count_dict.items(), key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ana2art = 'Ana2Articles'\n",
    "if not os.path.exists(ana2art):\n",
    "    os.mkdir(ana2art)\n",
    "with open(ana2art + '/sorted_sources.csv', 'w', newline='', ) as csvfile: # open the csvfile as a writeable file\n",
    "    fieldname = ['source', 'frequency']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldname) # uses dictWriter to make columns in the csv file\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in sorted_sources: # for to search through each dict pair in the list\n",
    "        writer.writerow({'source': i[0], 'frequency': i[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Analysis of Articles API:\n",
    "- This analysis was to see which section had the most articles written about it\n",
    "- There was a total of 20 sections that had articles written about them\n",
    "- The most commonly written about sections were: \"U.S.\", \"World\", and \"Business Day\"\n",
    "- CONCLUSION:\n",
    "    - \"Most readers care more about the U.S. and World News than they do about all other news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('U.S.', 54), ('World', 41), ('Business Day', 26), ('Sports', 18), ('Arts', 10), ('Books', 6), ('Real Estate', 5), ('Technology', 4), ('Opinion', 4), ('T Magazine', 4), ('Fashion & Style', 3), ('Food', 3), ('Watching', 3), ('Movies', 2), ('Science', 2), ('Health', 1), ('Times Insider', 1), ('N.Y. / Region', 1), ('The Upshot', 1), ('Theater', 1)]\n"
     ]
    }
   ],
   "source": [
    "section_name_list = list()\n",
    "for my_key in my_article_dict:\n",
    "    for doc in my_article_dict[my_key][\"response\"][\"docs\"]:\n",
    "        section_name_list.append(doc['section_name'])\n",
    "section_name_set = set(section_name_list)\n",
    "name_count = dict()\n",
    "for name in section_name_set:\n",
    "    name_count[name] = section_name_list.count(name)\n",
    "sorted_section_names = sorted(name_count.items(), key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ana3art = 'Ana3Articles'\n",
    "if not os.path.exists(ana3art):\n",
    "    os.mkdir(ana3art)\n",
    "with open(ana3art + '/sorted_section_names.csv', 'w', newline='', ) as csvfile: # open the csvfile as a writeable file\n",
    "    fieldname = ['country', 'frequency']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldname) # uses dictWriter to make columns in the csv file\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in sorted_section_names: # for to search through each dict pair in the list\n",
    "        writer.writerow({'country': i[0], 'frequency': i[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Archive Analysis\n",
    "- Second analysis of the archive files was to show zipfs law with stopwords\n",
    "- Conclusion:\n",
    "    - The graph that is saved as: zipfs_archives_2010-2017_withstopwords.png confirms that zipfs law still holds true.  After doing a log-log plot it shows that the most common word is used about twice as frequently as the next word and so on, until the end where all words are used once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_archives = glob ('Archive*') # get all article api json files\n",
    "#print (all_archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_archive_dict = dict() # dict for all json articles\n",
    "for archive in all_archives: # read in each file from the Archive list\n",
    "    file = open(archive).read()\n",
    "    json_file = json.loads(file)\n",
    "    my_archive_dict[file] = json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_snippet_dict = dict()\n",
    "for file in my_archive_dict: # pull out each file in the dictionary\n",
    "     for doc in my_archive_dict[file][\"response\"][\"docs\"]:\n",
    "            if not(doc['snippet'] == None):\n",
    "                my_snippet_dict[doc['headline']['main']] = (doc['snippet'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removePunctuation (a_str):\n",
    "    for a_char in a_str:\n",
    "        if a_char in punctuation:\n",
    "            a_str = a_str.replace(a_char, '')\n",
    "    return a_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_word_list = list()\n",
    "my_word_counter = dict()\n",
    "for snippet in my_snippet_dict: # go through each snippet in the dictionary\n",
    "    my_snippet_dict[snippet] = removePunctuation(my_snippet_dict[snippet]) # remove punctuation from each string\n",
    "    my_word_list.append(my_snippet_dict[snippet].split()) # add each word to the end of the list\n",
    "    \n",
    "word_frequency = collections.defaultdict(int)\n",
    "for x in itertools.chain.from_iterable(my_word_list):\n",
    "    word_frequency[x] += 1\n",
    "    \n",
    "sorted_word_count = sorted(word_frequency.items(), key=lambda pair: pair[1], reverse=True) # sort the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ana1arch = 'Ana1Archive'\n",
    "if not os.path.exists(ana1arch):\n",
    "    os.mkdir(ana1arch)\n",
    "counter = 1\n",
    "with open(ana1arch + '/zipfs_archives_2010-2017.csv', 'w', newline='', ) as csvfile: # open the csvfile as a writeable file\n",
    "    fieldname = ['word', 'rank', 'frequency']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldname) # uses dictWriter to make columns in the csv file\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in sorted_word_count: # for to search through each dict pair in the list\n",
    "        writer.writerow({'word': i[0], 'rank': counter, 'frequency': i[1]})\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Archive Analysis\n",
    "- Second analysis of the archive files was to show zipfs law without stopwords\n",
    "- Conclusion:\n",
    "    - The graph that is saved as: zipfs_archives_no_stopwords_2010-2017.png confirms that zipfs law still holds true.  After doing a log-log plot it shows that the most common word is used about twice as frequently as the next word and so on, until the end where all words are used once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_archives = glob ('Archive*') # get all article api json files\n",
    "#print (all_archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_archive_dict = dict() # dict for all json articles\n",
    "for archive in all_archives: # read in each file from the Archive list\n",
    "    file = open(archive).read()\n",
    "    json_file = json.loads(file)\n",
    "    my_archive_dict[file] = json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_word_list = list()\n",
    "my_word_counter = dict()\n",
    "for snippet in my_snippet_dict: # go through each snippet in the dictionary\n",
    "    my_snippet_dict[snippet] = removePunctuation(my_snippet_dict[snippet]) # remove punctuation from each string\n",
    "    my_word_list.append(my_snippet_dict[snippet].split()) # add each word to the end of the list\n",
    "\n",
    "my_word_list_no_stopwords = list()\n",
    "my_word_list_no_stopwords = removeStopWords(my_word_list)\n",
    "    \n",
    "word_frequency = collections.defaultdict(int)\n",
    "for x in itertools.chain.from_iterable(my_word_list_no_stopwords):\n",
    "    word_frequency[x] += 1\n",
    "    \n",
    "sorted_word_count = sorted(word_frequency.items(), key=lambda pair: pair[1], reverse=True) # sort the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuation (a_str):\n",
    "    for a_char in a_str:\n",
    "        if a_char in punctuation:\n",
    "            a_str = a_str.replace(a_char, '')\n",
    "    return a_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords (alist):\n",
    "    for my_list in alist:\n",
    "        if my_list:\n",
    "            for word in my_list: \n",
    "                if word in stopwords.words('english'):\n",
    "                    my_list.remove(word)\n",
    "    return alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ana2arch = 'Ana2Archive'\n",
    "if not os.path.exists(ana2arch):\n",
    "    os.mkdir(ana2arch)\n",
    "counter = 1\n",
    "with open(ana2arch + '/zipfs_archives_no_stopwords_2010-2017.csv', 'w', newline='', ) as csvfile: # open the csvfile as a writeable file\n",
    "    fieldname = ['word', 'rank', 'frequency']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldname) # uses dictWriter to make columns in the csv file\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in sorted_word_count: # for to search through each dict pair in the list\n",
    "        writer.writerow({'word': i[0], 'rank': counter, 'frequency': i[1]})\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Archive Analysis\n",
    "- Third analysis of the archive files was to count the number contributions each author made to see who deserves a raise\n",
    "- This analysis required looking at all archives from 2010-present and looking at all of the contributors in the byline\n",
    "- After getting the list of contributors, counting how many instances of their names was next.\n",
    "- CONCLUSION:\n",
    "    - Paul Krugman had his name against the most stories (665), so he is going to get a raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_archives = glob ('Archive*') # get all article api json files\n",
    "\n",
    "my_archive_dict = dict() # dict for all json articles\n",
    "for archive in all_archives: # read in each file from the Archive list\n",
    "    file = open(archive).read()\n",
    "    json_file = json.loads(file)\n",
    "    my_archive_dict[file] = json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_name_list = list()\n",
    "for file in my_archive_dict: # pull out each file in the dictionary\n",
    "     for doc in my_archive_dict[file][\"response\"][\"docs\"]:\n",
    "        if (doc['byline']) and (doc['byline']['person']):\n",
    "            for people in doc['byline']['person']:\n",
    "                if ('lastname' in people.keys()):\n",
    "                    my_name_list.append(people['firstname'].lower() + ' ' + people['lastname'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (my_name_list)\n",
    "name_set = set(my_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_count = dict()\n",
    "for name in name_set:\n",
    "    name_count[name] = my_name_list.count(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_name_count = sorted(name_count.items(), key=lambda pair: pair[1], reverse=True)\n",
    "ana3arch = 'Ana3Archive'\n",
    "if not os.path.exists(ana3arch):\n",
    "    os.mkdir(ana3arch)\n",
    "counter = 1\n",
    "with open(ana3arch + '/sorted_name.csv', 'w', newline='', ) as csvfile: # open the csvfile as a writeable file\n",
    "    fieldname = ['author', 'rank', 'contributions']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldname) # uses dictWriter to make columns in the csv file\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in sorted_name_count: # for to search through each dict pair in the list\n",
    "        writer.writerow({'author': i[0], 'rank': counter, 'contributions': i[1]})\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
